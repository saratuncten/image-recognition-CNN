{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition - Digit Recognizer - Model 3\n",
    "\n",
    "**All notebooks will have the same EDA and Preprocessing. The difference in each notebook will begin at section IV - Model Creation**\n",
    "\n",
    "\n",
    "I am doing the digit recognizer competition on Kaggle. This competiton is aimed at predicting digits (0-9) from images of handwritten numbers. We will be judged on accuracy in this competition. \n",
    "\n",
    "I will be building a variety of Convolutional Neural Network (CNN) models for this competition, these are the most commonly used model for image recognition. CNNs are a deep learning model that are similar to the ANN model we learned in class. CNNs include both an input an output layer, just like ANNs. CNNs also have hidden layers that can include pooling layers, convolutional layers, dense layers and normalization layers. I will get into more detail about these layers in my code examples for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources and links\n",
    "\n",
    "Competition - https://www.kaggle.com/c/digit-recognizer/data\n",
    "\n",
    "Documentation - https://keras.io/models/sequential/ , https://keras.io/getting-started/sequential-model-guide/ , https://towardsdatascience.com/build-your-own-convolution-neural-network-in-5-mins-4217c2cf964f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Data Prep\n",
    "\n",
    "**EDA and prep will not change between models. The Model Creation portion of this code is the only thing that will change throughout each notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data and create file for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in train data\n",
    "train = pd.read_csv(\"C:/Users/ST034045/OneDrive - Cerner Corporation/Documents/Rockhurst Data Science/predictivemodels2/final project/train.csv\", sep=',')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in test data\n",
    "test = pd.read_csv(\"C:/Users/ST034045/OneDrive - Cerner Corporation/Documents/Rockhurst Data Science/predictivemodels2/final project/test.csv\", sep=',')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create file for submission\n",
    "output = \"submissionCNN3.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 251.5 MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 785 entries (label + 784 pixel colummns) as expected and outline in the Kaggle data description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for NULLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       785\n",
       "unique        1\n",
       "top       False\n",
       "freq        785\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().any().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no NULLs we need to replace or remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = sns.countplot(train[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the digit drawings are evenly distributed throughout the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data to features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    train.iloc[:,1:].values, train.iloc[:,0].values, test_size=0.20, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8400, 784)\n",
      "(33600, 784)\n",
      "(8400,)\n",
      "(33600,)\n"
     ]
    }
   ],
   "source": [
    "#check the shapes\n",
    "print(X_val.shape)\n",
    "print(X_train.shape)\n",
    "print(Y_val.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN requires reshaping the data so it is viewed as a 28x28 square, so we can see the digit drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = plt.imshow(X_train[0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the first digit in our training data set is a 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape data\n",
    "\n",
    "Now I need to reshape all of the data so it's read the same way as the digit shown above. MNIST is a gray scale data set, which requires adding one dimension at the end (hence why we have it as (28x28x1). If the data was RBG, I would use a 3 instead of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_val = X_val.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize on 0-1 Scale\n",
    "The CNN can run on non-normalized data, however, changing it to a 0-1 scale will increase perfomance and the model will run faster than if the data remained 0-255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")/255.\n",
    "X_val = X_val.astype(\"float32\")/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode target variable\n",
    "\n",
    "We have to one-hot code the target variable. Instead of the label being a number between 0-9, this will convert each label to an array of 0s and 1s, with the 1 being in the position on the array that corresponds with the digit. \n",
    "\n",
    "For example: 2 becomes [0,0,1,0,0,0,0,0,0,0]. Printed example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "Y_train = to_categorical(Y_train)\n",
    "Y_val  = to_categorical(Y_val)\n",
    "\n",
    "print(Y_train[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set the batch_size, num_classes, and epochs before building or compiling the model.\n",
    "\n",
    "**Batch size** will define the number of samples that will be propogated through the network. I will change this from Model 2 to a larger number. Since the total number is samples I have is pretty high, I will not make the batch size as large as my sample size. https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
    "\n",
    "\n",
    "\n",
    "**Epochs** are how many times we will train the model. Also can be said as one forward pass and one backward pass of all the training examples. Lower epochs will help the model run much quicker, but will not get as high accuracy. Accuracy increases as you go through epochs.\n",
    "\n",
    "**Num classes** will stay at 10. This is how many classes there are to predict (0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100  #default is 32\n",
    "num_classes = 10\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the sequential model with convolutional layers and max pooling added. \n",
    "\n",
    "**MaxPooling layer** is a form of subsampling. In the pooling layer, the pooling size is determined. Max pooling takes the maximum value from a neighborhood of pixels. This can aid in preventing or reducing overfitting.\n",
    "\n",
    "**Dropout layer** will reduce overfitting by randomly switching off neurons in the network and guiding the data to find new paths. The goal of dropout layers is to reduce model complexity. If you set half of activtations to zero, the neural network cannot use those activations in the foward passes during training.\n",
    "\n",
    "In this model the first dropout layer randomly deactivates 25%. The second deactivates 50%.\n",
    "https://www.quora.com/In-Keras-what-is-a-dense-and-a-dropout-layer\n",
    "\n",
    "**Flatten layer** Flattening is the process of converting all the resultant 2 dimensional arrays into a single long continuous linear vector.\n",
    "https://www.quora.com/What-is-the-meaning-of-flattening-step-in-a-convolutional-neural-network\n",
    "\n",
    "\n",
    "**Dense** layers are used for class prediction.  This is the classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential()\n",
    "#1st convolutional layer with 16 nodes and a kernel size (filter matrix) of 5x5\n",
    "model.add(Conv2D(16, kernel_size=(5, 5),\n",
    "                 activation='relu', #(rectified linear activation) works well in neural networks\n",
    "                 input_shape=(28,28,1))) #only need to specify input shape in 1st layer\n",
    "\n",
    "#2nd convolutional layer with 32 nodes and a kernel size of 3x3\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(3, 3))) #subsampling layer\n",
    "model.add(Dropout(0.25)) #regularization to reduce overfitting\n",
    "model.add(Flatten()) #flatten layer to single long vector to prep for classification in dense layer\n",
    "model.add(Dense(100, activation='relu')) #classifier layer\n",
    "model.add(Dropout(0.5)) #regularization\n",
    "model.add(Dense(num_classes, activation='softmax')) \n",
    "#softmax makes output sum to 1 so they can be interpreted as probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll compile the model. This takes into account three parameters:\n",
    "\n",
    "**optimizer** - controls our learning rate. From towardsdatascience - The learning rate determines how fast the optimal weights for the model are calculated. A smaller learning rate may lead to more accurate weights (up to a certain point), but the time it takes to compute the weights will be longer. \"adam\" optimizer adjusts the learning rate throughout training - will try adam in model 4.\n",
    "\n",
    "**loss** - categorical crossentropy is the most common loss function to use for classification purposes.\n",
    "\n",
    "**metrics** - I will use the accuracy metric, this is what the Kaggle competition uses to evaluate my model's performance.\n",
    "\n",
    "https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/15\n",
      "33600/33600 [==============================] - 79s 2ms/step - loss: 0.4205 - acc: 0.8682 - val_loss: 0.0909 - val_acc: 0.9737\n",
      "Epoch 2/15\n",
      "33600/33600 [==============================] - 79s 2ms/step - loss: 0.1449 - acc: 0.9574 - val_loss: 0.0616 - val_acc: 0.9808\n",
      "Epoch 3/15\n",
      "33600/33600 [==============================] - 76s 2ms/step - loss: 0.1083 - acc: 0.9680 - val_loss: 0.0502 - val_acc: 0.9827\n",
      "Epoch 4/15\n",
      "33600/33600 [==============================] - 78s 2ms/step - loss: 0.0899 - acc: 0.9729 - val_loss: 0.0481 - val_acc: 0.9846\n",
      "Epoch 5/15\n",
      "33600/33600 [==============================] - 85s 3ms/step - loss: 0.0823 - acc: 0.9760 - val_loss: 0.0411 - val_acc: 0.9865\n",
      "Epoch 6/15\n",
      "33600/33600 [==============================] - 87s 3ms/step - loss: 0.0752 - acc: 0.9785 - val_loss: 0.0403 - val_acc: 0.9873\n",
      "Epoch 7/15\n",
      "33600/33600 [==============================] - 81s 2ms/step - loss: 0.0680 - acc: 0.9795 - val_loss: 0.0387 - val_acc: 0.9875\n",
      "Epoch 8/15\n",
      "33600/33600 [==============================] - 88s 3ms/step - loss: 0.0640 - acc: 0.9812 - val_loss: 0.0374 - val_acc: 0.9885\n",
      "Epoch 9/15\n",
      "33600/33600 [==============================] - 85s 3ms/step - loss: 0.0588 - acc: 0.9818 - val_loss: 0.0384 - val_acc: 0.9882\n",
      "Epoch 10/15\n",
      "33600/33600 [==============================] - 82s 2ms/step - loss: 0.0546 - acc: 0.9836 - val_loss: 0.0371 - val_acc: 0.9883\n",
      "Epoch 11/15\n",
      "33600/33600 [==============================] - 80s 2ms/step - loss: 0.0527 - acc: 0.9847 - val_loss: 0.0339 - val_acc: 0.9895\n",
      "Epoch 12/15\n",
      "33600/33600 [==============================] - 92s 3ms/step - loss: 0.0487 - acc: 0.9845 - val_loss: 0.0358 - val_acc: 0.9889\n",
      "Epoch 13/15\n",
      "33600/33600 [==============================] - 89s 3ms/step - loss: 0.0467 - acc: 0.9854 - val_loss: 0.0377 - val_acc: 0.9874\n",
      "Epoch 14/15\n",
      "33600/33600 [==============================] - 88s 3ms/step - loss: 0.0482 - acc: 0.9858 - val_loss: 0.0331 - val_acc: 0.9887\n",
      "Epoch 15/15\n",
      "33600/33600 [==============================] - 77s 2ms/step - loss: 0.0426 - acc: 0.9874 - val_loss: 0.0347 - val_acc: 0.9888\n",
      "Test loss: 0.03469731071121281\n",
      "Test accuracy: 0.9888095238095238\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='keras.optimizers.Adadelta()',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, Y_val))\n",
    "score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.0347, final accuracy: 0.9888\n"
     ]
    }
   ],
   "source": [
    "final_loss, final_acc = model.evaluate(X_val, Y_val, verbose=0)\n",
    "print(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final overall accuracy is about 98.88%, this is good, but is not as high as the 99.7% achieved in the example Model 1 I first looked at. I will try to improve this in further models.\n",
    "\n",
    "Confusion matrix below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[845   0   0   0   0   0   3   0   2   0]\n",
      " [  0 900   2   0   1   0   0   1   1   0]\n",
      " [  0   0 829   0   1   0   0   2   2   1]\n",
      " [  0   0   1 825   0  10   0   1   1   1]\n",
      " [  0   1   0   0 796   0   0   2   1   6]\n",
      " [  0   0   0   1   0 778   0   0   0   0]\n",
      " [  1   1   1   0   0   0 842   0   1   0]\n",
      " [  0   1   5   0   0   0   0 849   1   2]\n",
      " [  2   4   2   0   3   1   1   1 805   2]\n",
      " [  4   1   0   1   4   2   0   7   5 837]]\n"
     ]
    }
   ],
   "source": [
    "Y_hat = model.predict(X_val)\n",
    "y_pred = np.argmax(Y_hat, axis=1)\n",
    "y_true = np.argmax(Y_val, axis=1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CNN model actually did better than Model 1 and 2 on the \"4\" vs. \"9\" problem. It is guessing fewer \"9s\" as \"4s\" than previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit\n",
    "\n",
    "Now I'll run this model on the test data and create a submission csv with my predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_testset1 = test\n",
    "x_test1 = mnist_testset1.astype(\"float32\")\n",
    "x_test1 = x_test1.values.reshape(-1, 28, 28, 1)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(x_test1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_hat,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output, 'w') as f :\n",
    "    f.write('ImageId,Label\\n')\n",
    "    for i in range(len(y_pred)) :\n",
    "        f.write(\"\".join([str(i+1),',',str(y_pred[i]),'\\n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
